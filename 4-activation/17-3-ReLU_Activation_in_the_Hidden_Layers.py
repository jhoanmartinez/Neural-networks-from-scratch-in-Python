"""ReLU Activation in the Hidden Layers


    El bias es donde empieza la funcion y el peso
    es la inlinacion (?)


    La capa de salida usa la funcion de activacion Lineal, y las capas
    ocultasusaran la funcion de activacion lineal recitificada (ReLU) 

    pag 90
    """

